# トラブルシューティング

## NaN detected in...って出た！！

SDXLなら、詳細設定->パスでVAEに https://huggingface.co/madebyollin/sdxl-vae-fp16-fix で配布されているsdxl.vae.safetensorsを選択すると直るかも。

それがダメなら詳細設定->パフォーマンスにあるVAEを32ビットで使用にチェックをつける。

## CUDA error: Out Of Memoryって出た！なにこれ？

グラフィックボードのメモリが不足している。

まずは以下のことをやってみる。

バッチサイズを下げる、latentのキャッシュを有効にする、不要なアプリは終了する。

それでもダメなら、gradient_checkpointing使用する。SDXLは必須。

メモリのフォールバックを有効にする手もある(動くが遅い)。

大容量データの転送が多いAI用途ではPCIeは完全に足手まといなっている。

## 速度が極端に遅い！

グラフィックボードのメモリが不足している可能性がある。

タスクマネージャーのパフォーマンスタブのGPUが以下のようになってるならVRAM不足。

※古いWindowsのタスクマネージャーのGPU使用率はレンダリングの負荷なのであてにならない
![ss184846](https://github.com/RedRayz/Kohya_lora_param_gui/assets/71994877/885dd37a-943a-443e-b0d5-dd98b6b6d9e7)

GPU-ZやMSI Afterburnerのグラフを見るのもあり。それのグラフでVRAM使用量が上限に近い状態で、GPU使用率100%張り付きで、消費電力が定格より大幅に低くなっているならVRAM不足。

## VRAMが溢れても遅くならないこともある

多少共有メモリを使用していても、GPUのバスインターフェースの使用率が0%前後で消費電力が十分に高いなら速度低下はほとんどない。

#  ヒントとか

※経験に基づく主観的な内容が含まれます。参考程度にとどめてください。

## 次元数(DimまたはRank)は高いほうがいいの？

高いほど学習能力が上昇するが学習・生成が遅くなる。また高くしても細部が良くなるわけではない。高いほど生成時に崩壊しやすい。

基本的にSD1は16でいい。SDXLは性能とファイルサイズの観点から16以下を推奨。

## データセットについて

低品質な画像や学習に適さない画像は除去しておくといい。量と質を両立すべき。無理なら量より質。

## アルファのおすすめの値は？

dimの半分以下。高いほど生成時に崩壊しやすくなる。低すぎると学習能力が若干低下する。

## mean ar errorってなに？

画像リサイズ時のアスペクト比の誤差。0-1の範囲で示される。プログラムのエラーではない。

## データローダーのCPUスレッド数を上げると速くなる？

多くの処理がシングルスレッドで動作するためほとんど変わらない。最新世代のCPUに変えたほうが速くなる。

上げてもメモリ消費が増えるだけっぽい。1でいい。

## Optimizerはどれがいい？

RAdamScheduleFreeをおすすめする。LRは0.0004でも平気で、スケジューラーはconstantを使えてwarmupやrestart不要で楽。

AdaFactorはLoRA学習ではAdamWより遅く、メリットはない。あれは大規模学習向け。

## おすすめのステップ数は?

AdamWでキャラが3000-6000、画風が5000-10000、構図,シチュエーション,ポーズは9000以上。

SDXLでProdigyなら2000-3000ステップでよい。

CAMEやScheduleFreeなら800-1500ステップがいい感じ。

## SD1/2が苦手な物

図形のような正確な描画を求められるもの(ヘイロー、文字など)。武器などの細長い手持ちの物体。

これらはSDXLで改善している。パラメータ数0.9Bしかない旧型のSD1ごときには無理。

## ゲームのスクショを用いたLoRAで画風まで学習してほしくない

ベースモデルと教師画像の画風に差があるから学習している。

先にゲームのスクショで画風LoRAを作成し、モデルにマージしてできたモデルで学習しよう。

「モデルと教師画像の差分を学習する」ことを念頭に置いて実践するといい。

## no module named 'triton'は無視でおｋ

古いxformersで表示されることがあるが、なんの問題ない。

気になるなら、venv内で`pip install triton-windows`してWindows版tritonをインストールすることで消える。

## lossは小さいほうがいいの？

答えはNO。小さいほど教師画像に近いこと(=過学習)を意味するので基本的にあてにならない。コピー機のような少数の画像でやる時以外はlossが減少せずとも問題なし。

lossが上下に振れるのが普通。振れ方はseed値によって変化すると思われる。

lossの急上昇や上昇し続けるときは発散の兆候。上昇を続けてavr_lossがnanになったら学習は失敗。発散を避けるにはLRを下げよう。

v-predictionモデルは0.5前後でも正常。

## LoRAを使用すると品質が低下する

LR、ステップ数、Dim(Rank)のいずれかを下げると改善するかも。

しかし、LoRAの"ウェイトを二つの行列A,Bに分解して学習する"仕様が災いして何をしても品質を改善できないこともある。

これは行列Aを学習しないLoRA-FAを使えば(解決はしないが)改善する。

## LR Schedulerについて
LR調整アルゴリズム。
### cosine_with_restarts
推奨。num_lr_scheduler_cycleで指定した分だけコサイン波を繰り返す。warmupが指定されているなら0から始まりwarmupまで線形でLRを上げる。
### cosine
コサイン波形で減衰
### linear
線形に減衰。
### constant_with_warmup
warmupまで線形でLRを上げた後は一定。非推奨。ScheduleFree系向け。
### constant
LRは常に一定。RAdamScheduleFree以外は非推奨。
### polynomial
多項式。powerの値で減衰速度が変化する。

## GUIで表示される総ステップ数と実際の総ステップ数がずれる
Bucketingが有効だと複数のBucketを1バッチにまとめられない影響でずれる場合あり。

## LoRAのファイルサイズに影響する設定
ネットワーク次元数(DimまたはRank)、Conv2d拡張の使用(使用時はconv dimも)、LyCORIS使用時はLyCORISのアルゴリズムの種類、Unet/Tencの有無、保存時の精度(--save_precision)、モデルの種類

## 作ったLoRAを使うと出力が暗くなったり赤みがかった色合いになる
詳細設定->ノイズ関連->ノイズオフセットに学習元モデルで使用されたnoise_offsetの値を設定すると改善するかも。

Animagine-XL系で使用された値は`0.0357`。

Illustrious-XLは不明だが、`0.0357`あたりがちょうどいい？

Ponyで使用された値は不明だが、明暗に弱いあたり未使用と思われる。

# その他(学習以外)

## GrabberでGelbooruの画像やタグを正しく取得できない

今年6月から、GelbooruのAPIアクセスにはAPIキー(要アカウント)が必要だ。

以下の手順でユーザーIDとAPIキーを取得してGrabberに設定する。

1. GelbooruのWebサイトでログイン(アカウントが無いなら作成)する。

2. Settingsページ下部にあるOptionsのページを開く。

3. ページ下部にあるAPI Access Credentialsに表示されているAPIキーとユーザーIDをメモする。`&api_key=`と`&user_id=`の部分は不要。

4. Grabberを開き、gelbooruのタブを表示してからウィンドウ左下にあるソース(O)からgelbooru.comの設定を開く。

5. 「ログイン」タブで「タイプ」を「Through URL」にし、先ほどメモしたユーザIDとAPIキーを入力する。

6.  確認、OKを押して設定を閉じたら完了。ウィンドウ右上にあるOkを押すと画像とタグを取得するはず。

注意: Gelbooruは「ウマ娘」の検索にCloudflareの認証を要求するためGrabberでは表示できない。

## GrabberでDanbooruを閲覧できない

Webブラウザ以外からのアクセスをブロックしているのであきらめること。

## 学習阻害を目的とした渦状のノイズ(Adversarial Noise, いわゆるGlaze)のついた画像は避けるべき？
目立つレベルのノイズでなければデータセットに多少入っていても問題なし。気になるなら弱めのi2iかツールでぼかしをかけると良い。	ちなみに、Galzeはほとんど効果がない。
