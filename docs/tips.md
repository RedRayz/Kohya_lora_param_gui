#  ヒントとか

※経験に基づく主観的な内容が含まれます。参考程度にとどめてください。

## 次元数(DimまたはRank)は高いほうがいいの？

高いほど学習能力が上昇するが学習・生成が遅くなる。また高くしても細かい部分が良くなるわけではない。高いほど生成時に崩壊しやすい。

キャラは64、画風は16以下でいい。

## アルファのおすすめの値は？

dimの半分以下。高いほど生成時に崩壊しやすくなる。低すぎると学習能力が若干低下する。

## mean ar errorってなに？

画像リサイズ時のアスペクト比の誤差。0-1の範囲で示される。プログラムのエラーではない。

## データローダーのCPUスレッド数を上げると速くなる？

学習がシングルスレッド性能に強く依存してるためかほとんど変わらない。新しいCPUに変えたほうが速くなる。

上げてもメモリ消費が増えるだけっぽい。2でいい。

## Optimizerはどれがいい？

AdamWが無難。特別高性能ではないが、癖がなく失敗が少ない。再現度が欲しいならDAdaptLionがおすすめ。これも癖が少なめでいい。

Prodigyは学習結果に癖があるように感じる。

AdaFactorはLoRA学習で特にメリットはない。

画風は癖のないAdamWがいい感じだった。

## キャプショニングはどうする？

https://rentry.co/irir_lora#%E3%82%BF%E3%82%B0%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6

## おすすめのステップ数は？

キャラが3000-7000、画風が5000-10000、構図,シチュエーション,ポーズは9000以上。

## CUDA error: Out Of Memoryって出た！なにこれ？

グラフィックボードのメモリが不足している。

まずは以下のことをやってみる。

バッチサイズを下げる、cache_latentsを有効にする、不要なアプリは終了する。

それでもダメなら、解像度を下げるか、gradient_checkpointing使用する。

それでもダメなら・・・クロスアテンションの最適化にxformersではなくmem_eff_attnを使用する。

## 「Setting different lr values in different parameter groups」なんとかって何？

日本語にすると、「異なるパラメータグループ内で指定できる異なるLRの値は0のみとなります」とのこと。

一部のDAdaptation系のオプティマイザでは、UNetとTextEncoderの個別のLR設定や層別学習で0と1以外の設定はできない。

## SD1.Xおよび2.Xが苦手な物

図形のような正確な描画を求められるもの(ヘイロー、文字など)。武器などの細長い手持ちの物体。

OptimizerをDAdaptation系にするとマシになるが崩壊しやすいのは変わらない。

これらはSDXLで改善している。SD1.Xは記憶力不足なのかな？

## ゲームのスクショを用いたLoRAで画風まで学習してほしくない

ベースモデルと教師画像の画風に差があるから学習している。

先にゲームのスクショで画風LoRAを作成し、モデルにマージしてできたモデルで学習しよう。

「モデルと教師画像の差分を学習する」ことを念頭に置いて実践するといい。

## 学習モデルはなにがいいかな(イラスト)？

animefull(NAI)がいい。データの多様性が高く、意図しない要素の学習が少ない。

モデルマージ前提で画風ならマージ元にするモデルで学習する。

## no module named 'triton'は無視でおｋ

tritonはWindows向けビルドが無いので使用できないしなくても問題ない。あったところで性能に影響を与えない。

## lossは小さいほうがいいの？

答えはNO。小さいほど教師画像に近いこと(=過学習)を意味するので基本的にあてにならない。コピー機のような少数の画像でやる時以外はわずかに減少する程度で問題なし。

lossが増加し続けるのは悪い兆候。増加し続けると最終的に発散してノイズ製造機になる。発散を避けるにはLRを下げよう。