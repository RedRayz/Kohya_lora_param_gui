#  ヒントとか

※経験に基づく主観的な内容が含まれます。参考程度にとどめてください。

## 次元数(DimまたはRank)は高いほうがいいの？

高いほど学習能力が上昇するが学習・生成が遅くなる。また高くしても細部が良くなるわけではない。高いほど生成時に崩壊しやすい。

基本的に16でいい。SDXLは性能とファイルサイズの観点から8～16を推奨。

## アルファのおすすめの値は？

dimの半分以下。高いほど生成時に崩壊しやすくなる。低すぎると学習能力が若干低下する。

## mean ar errorってなに？

画像リサイズ時のアスペクト比の誤差。0-1の範囲で示される。プログラムのエラーではない。

## データローダーのCPUスレッド数を上げると速くなる？

多くの処理がシングルスレッドで動作するためほとんど変わらない。最新世代のCPUに変えたほうが速くなる。

上げてもメモリ消費が増えるだけっぽい。2でいい。

## Optimizerはどれがいい？

AdamWが無難。特別高性能ではないが、癖がなく失敗が少ない。再現度が欲しいならDAdaptLionがおすすめ。これも癖が少なめでいい。

Prodigyは学習結果に癖があるように感じる。

AdaFactorはLoRA学習で特にメリットはない。がSDXLでは使ってもいいいかも？

画風は癖のないAdamWがいい感じだった。

## おすすめのステップ数は？

キャラが3000-7000、画風が5000-10000、構図,シチュエーション,ポーズは9000以上。

## CUDA error: Out Of Memoryって出た！なにこれ？

グラフィックボードのメモリが不足している。

まずは以下のことをやってみる。

バッチサイズを下げる、cache_latentsを有効にする、不要なアプリは終了する。

それでもダメなら、解像度を下げるか、gradient_checkpointing使用する。

## 速度が極端に遅い！

グラフィックボードのメモリが不足している可能性がある。

タスクマネージャーのパフォーマンスタブのGPUが以下のようになってるならVRAM不足。

※タスクマネージャーのGPU使用率は2D/3Dレンダリングの負荷なのであてにならない
![ss184846](https://github.com/RedRayz/Kohya_lora_param_gui/assets/71994877/885dd37a-943a-443e-b0d5-dd98b6b6d9e7)

GPU-ZやMSI Afterburnerのグラフを見るのもあり。それのグラフでVRAM使用量が上限に近い状態で、GPU使用率100%張り付きで、消費電力が定格より大幅に低くなってるならVRAM不足。

## 「Setting different lr values in different parameter groups」なんとかっていうエラーは何？

日本語にすると、「異なるパラメータグループ内で指定できる異なるLRの値は0のみとなります」とのこと。

一部のDAdaptation系のオプティマイザでは、UNetとTextEncoderの個別のLR設定や層別学習で0と1以外の設定はできない。

## SD1.Xおよび2.Xが苦手な物

図形のような正確な描画を求められるもの(ヘイロー、文字など)。武器などの細長い手持ちの物体。

OptimizerをDAdaptation系にするとマシになるが崩壊しやすいのは変わらない。

これらはSDXLで改善している。SD1.Xごときには無理。

## ゲームのスクショを用いたLoRAで画風まで学習してほしくない

ベースモデルと教師画像の画風に差があるから学習している。

先にゲームのスクショで画風LoRAを作成し、モデルにマージしてできたモデルで学習しよう。

「モデルと教師画像の差分を学習する」ことを念頭に置いて実践するといい。

## 学習モデルはなにがいいかな(イラスト)？

animefull(NAI)がいい。データの多様性が高く、意図しない要素の学習が少ない。

SDXLならAnimagine-XL-3.0がいい感じ。

モデルマージ前提で画風ならマージ元にするモデルで学習する。

## no module named 'triton'は無視でおｋ

tritonはWindows向けビルドが無いので使用できないしなくても問題ない。

## lossは小さいほうがいいの？

答えはNO。小さいほど教師画像に近いこと(=過学習)を意味するので基本的にあてにならない。コピー機のような少数の画像でやる時以外はわずかに減少する程度で問題なし。

lossが上下に振れるのが普通。振れ方はseed値によって変化すると思われる。

lossの急上昇は発散の兆候。上昇を続けてlossがNaNになったら学習は失敗。発散を避けるにはLRを下げよう。

## LR Schedulerについて
LR調整アルゴリズム。
- cosine_with_restarts
推奨。
num_lr_scheduler_cycleで指定した分だけコサイン波を繰り返す。warmupが指定されているなら0から始まりwarmupまで線形でLRを上げる。
- cosine
コサイン波形で減衰
- linear
線形に減衰。
- constant_with_warmup
warmupまで線形でLRを上げた後は一定。非推奨。
- constant
LRは常に一定。非推奨。