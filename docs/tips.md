#  ヒントとか

※経験に基づく主観的な内容が含まれます。参考程度にとどめてください。

## 次元数(DimまたはRank)は高いほうがいいの？

高いほど学習能力が上昇するが学習・生成が遅くなる。また高くしても細かい部分が良くなるわけではない。高いほど生成時に崩壊しやすい。

キャラは64、画風は16以下でいい。

## アルファのおすすめの値は？

dimの半分以下。高いほど生成時に崩壊しやすくなる。低すぎると学習能力が若干低下する。

## mean ar errorってなに？

画像リサイズ時のアスペクト比の誤差。0-1の範囲で示される。プログラムのエラーではない。

## データローダーのCPUスレッド数を上げると速くなる？

学習がシングルスレッド性能に強く依存してるためかほとんど変わらない。新しいCPUに変えたほうが速くなる。

上げてもメモリ消費が増えるだけっぽい。2でいい。

## Optimizerはどれがいい？

AdamWが無難。特別高性能ではないが、癖がなく失敗が少ない。再現度が欲しいならDAdaptLionがおすすめ。これも癖が少なめでいい。

Prodigyは学習結果に癖があるように感じる。

AdaFactorはLoRA学習で特にメリットはない。

## キャプショニングはどうする？

https://rentry.co/irir_lora#%E3%82%BF%E3%82%B0%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6

## おすすめのステップ数は？

キャラが3000-6000、画風が5000-10000、構図,シチュエーション,ポーズは9000以上。

## CUDA error: Out Of Memoryって出た！なにこれ？

グラフィックボードのメモリが不足している。

まずは以下のことをやってみる。

バッチサイズを下げる、cache_latentsを有効にする、不要なアプリは終了する。

それでもダメなら、解像度を下げるか、gradient_checkpointing使用する。

## 「Setting different lr values in different parameter groups」なんとかって何？

一部のDAdaptation系のオプティマイザでは、UNetとTextEncoderの個別のLR設定や層別学習で0と1以外の設定はできない。

## SD1.Xおよび2.Xが苦手な物

複雑かつ決められた形状(ヘイロー、文字など)。武器などの細長い手持ちの物体。

OptimizerをDAdaptation系にするとマシになるが崩壊しやすいのは変わらない。

これらはSDXLで改善している。

## ゲームのスクショを用いたLoRAで画風まで学習してほしくない

ベースモデルと教師画像の画風に差があるから学習している。

先にゲームのスクショで画風LoRAを作成し、モデルにマージしてできたモデルで学習しよう。

「モデルと教師画像の差分を学習する」ことを念頭に置いて実践するといい。

## 学習モデルはなにがいいかな(イラスト)？

animefull(NAI)がいい。データの多様性が高く、意図しない要素の学習が少ない。

モデルマージ前提で画風ならマージ元にするモデルで学習する。